{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/drive2/ryusejong/miniconda3/envs/llm1/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json \n",
    "import time \n",
    "import re\n",
    "import random\n",
    "import numpy as np \n",
    "from tqdm.auto import tqdm\n",
    "from util.utils import set_seed, read_data, save_result, get_answer_from_text, chat_huggingface\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "\n",
    "seed = 42\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sample Failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7473 samples\n"
     ]
    }
   ],
   "source": [
    "path_input = \"dataset/GSM8K_train.jsonl\"\n",
    "path_result = \"output/GSM8K_Llama-3-8B-Instruct_zeroshot_CoT_train.jsonl\"\n",
    "\n",
    "data = read_data(path_input)\n",
    "results = read_data(path_result)\n",
    "\n",
    "sample_portion = 1.0  # Set the portion of the dataset \n",
    "sample_k = int(np.floor(sample_portion * len(results)))\n",
    "sample_indices = np.random.choice(np.arange(len(data)), size=sample_k, replace=False)\n",
    "\n",
    "path_output = f\"failure/GSM8K_Llama-3-8B-Instruct_zeroshot_CoT_train_seed{seed}_portion{sample_portion}.jsonl\"\n",
    "print(f\"{len(sample_indices)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7473/7473 [00:00<00:00, 550532.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 7473, Failures: 1751, Percentage: 23.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "path_tensor = f\"memory/GSM8K_Llama-3-8B-Instruct_zeroshot_CoT_train_seed{seed}_portion{sample_portion}.pt\"\n",
    "path_revision = f\"memory/GSM8K_Llama-3-8B-Instruct_zeroshot_CoT_train_seed{seed}_portion{sample_portion}.jsonl\"\n",
    "\n",
    "i = 0\n",
    "fail_list = []\n",
    "for index in tqdm(sample_indices):\n",
    "    answer = results[index].get(\"answer\", \"\")\n",
    "    pred_ans = results[index].get(\"pred_ans\", \"\")\n",
    "    \n",
    "    if answer != pred_ans:\n",
    "        fail_list.append({\"index\": int(index),\n",
    "                          \"question\": data[results[index].get(\"index\", 0)][\"question\"],\n",
    "                          \"answer\": answer,\n",
    "                          \"reasnoing\": data[results[index].get(\"index\", 0)][\"reasoning\"],\n",
    "                          \"fail_answer\": pred_ans,\n",
    "                          \"fail_reasnoing\": results[index].get(\"A\", \"\").get(\"content\", \"\"),\n",
    "                          \"tensor_index\": i, \n",
    "                          \"tensor_path\": path_tensor,\n",
    "                          \"revision_path\": path_revision})\n",
    "        i += 1\n",
    "\n",
    "print(f\"Total samples: {len(sample_indices)}, Failures: {len(fail_list)}, Percentage: {len(fail_list) / len(sample_indices) * 100:.2f}%\")\n",
    "\n",
    "for fail in fail_list:\n",
    "    save_result(fail, path_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get Advice from GPT-o3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__< Used Prompt ( + failure output .jsonl file) >__\n",
    "\n",
    "Each dictionary has a correct \"reasoning\" and \"answer\" for the \"question\" and a \"fail_reasoning\" and \"fail_answer\" that the model incorrectly generated. \n",
    "\n",
    "\"answer\" 10086100100.0 means no answer was given.\n",
    "\n",
    "For each question,  please give advice on how to correct the errors in fail_reasoning compared to reasoning so that an answer other than fail_answer can be derived.  \n",
    "\n",
    "Advice include effective methods to make question easier to deduce the correct answer without mentioning answer, fail_answer and numbers in reasoning and fail reasoning directly. \n",
    "\n",
    "Also please give as variable advice as possible. \n",
    "\n",
    "Give the generated advice in the form of a .jsonl file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract Fail Question Latent Vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Llama-3-8B-Instruct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 81.70it/s]\n"
     ]
    }
   ],
   "source": [
    "HUGGINGFACE_TOKEN = \"XXX\"\n",
    "\n",
    "model_path = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, token=HUGGINGFACE_TOKEN)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    token=HUGGINGFACE_TOKEN,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "model.generation_config.temperature=None\n",
    "model.generation_config.top_p=None\n",
    "model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1751/1751 [01:10<00:00, 24.72it/s]\n",
      "/tmp/ipykernel_4311/1141505669.py:19: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
      "  torch.save(torch.tensor(fail_question_vectors), path_tensor)\n"
     ]
    }
   ],
   "source": [
    "# extract the last latent representation(vector) from the question\n",
    "fail_question_vectors = []\n",
    "\n",
    "for fail in tqdm(fail_list):\n",
    "    question = fail[\"question\"]\n",
    "    inputs = tokenizer(question, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "        hidden_states = outputs.hidden_states[-1]  # Get the last layer's hidden states\n",
    "        hidden = hidden_states.mean(dim=1).squeeze(0)      # -> [dim], still bfloat16\n",
    "        hidden = hidden.to(torch.float32)                  # bfloat16 → float32\n",
    "        question_vector = hidden.cpu().numpy()  \n",
    "        #question_vector = hidden_states.mean(dim=1).squeeze().cpu().numpy()  # Average pooling\n",
    "        \n",
    "    fail_question_vectors.append(question_vector)\n",
    "    \n",
    "# Save the question vectors to a tensor file\n",
    "torch.save(torch.tensor(fail_question_vectors), path_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.Size([1751, 4096]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "fail_memories = torch.load(path_tensor)\n",
    "print(type(fail_memories), fail_memories.shape, fail_memories.dtype)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm1",
   "language": "python",
   "name": "llm1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
